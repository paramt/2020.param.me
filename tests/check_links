#!/usr/bin/env python
# -*- coding: UTF-8 -*-

# These URLs are blacklisted because they don't send back proper responses to automated scripts
blacklisted = ["https://www.reddit.com/r/MemeEconomy", 
               "https://www.linkedin.com/in/paramt/", 
               "https://github.com/paramt/paramt.github.io/actions", 
               "https://www.instagram.com/xparam/", 
               "https://twitter.com/paramoham",
              ]

import csv
from urlextract import URLExtract
import requests

projects_file = "_data/projects.csv"
projects_full_file = "_data/projects-full.csv"
achievements_file = "_data/achievements.csv"
timeline_file = "_data/timeline.csv"
timeline_markdown_file = "timeline/README.md"
projects_markdown_file = "projects/README.md"
markdown_file = "README.md"

links = []
projects = []
projects_full = []
achievements = []
timeline = []
exit_status = 0

print(f"Collecting URLs from {projects_file}")

with open(projects_file) as f:
    lines = f.readlines()

    for l in  csv.reader(lines, quotechar='"', delimiter=',', quoting=csv.QUOTE_ALL, skipinitialspace=True):
        projects.append(l[4])

    projects.pop(0)

links += projects

print(f"Collecting URLs from {projects_full_file}")

with open(projects_file) as f:
    lines = f.readlines()

    for l in  csv.reader(lines, quotechar='"', delimiter=',', quoting=csv.QUOTE_ALL, skipinitialspace=True):
        projects_full.append(l[4])

    projects_full.pop(0)

links += projects_full

print(f"Collecting URLs from {achievements_file}")

with open(achievements_file) as f:
    lines = f.readlines()

    for l in  csv.reader(lines, quotechar='"', delimiter=',', quoting=csv.QUOTE_ALL, skipinitialspace=True):
        achievements.append(l[4])

    achievements.pop(0)

links += achievements

print(f"Collecting URLs from {markdown_file}")

with open(markdown_file) as f:
    text = f.read()

extractor = URLExtract()
markdown = extractor.find_urls(text)

links += markdown

print(f"Collecting URLs from {timeline_markdown_file}")

with open(timeline_markdown_file) as f:
    text = f.read()

extractor = URLExtract()
timeline_markdown = extractor.find_urls(text)

links += timeline_markdown

print(f"Collecting URLs from {projects_markdown_file}")

with open(projects_markdown_file) as f:
    text = f.read()

extractor = URLExtract()
projects_markdown = extractor.find_urls(text)

links += projects_markdown

print("Cleaning up list of URLs")

# Remove mailto links
links = [url for url in links if "mailto://" not in url]

# Remove blacklisted links
for link in links:
    if link in blacklisted:
        links.remove(link)
        print(f"Removed {link}")

print("Checking all URLs")

for url in links:
    try:
        request = requests.get(url)
        if request.status_code == 200:
            print(f"✓ 200 {url}")
        elif request.status_code >= 400:
            print(f"✕ {request.status_code} {url}")
            exit_status = 1
        else:
            print(f"⚪ {request.status_code} {url}")

    except:
        print(f"✕ ERR {url}")

        # Continue through all URLs but fail test at the end
        exit_status = 1
        continue

exit(exit_status)
